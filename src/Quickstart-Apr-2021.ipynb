{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_rPkGOQ2x81",
        "outputId": "b75b5b96-4e0f-47e9-c7ab-08640e070326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=0\n"
          ]
        }
      ],
      "source": [
        "\"\"\"To profile the running time line-by-line, CUDA needs to run synchronously. Set this to 1 to do that\"\"\"\n",
        "%env CUDA_LAUNCH_BLOCKING=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXH6y_rftSeK"
      },
      "source": [
        "#### Install Optional Dependencies for Line Profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpoUbaRkYaf9"
      },
      "outputs": [],
      "source": [
        "# # %%capture\n",
        "# \"\"\"For profiling how long each line takes, both on CPU and GPU (needs synchronization)\"\"\"\n",
        "# !pip install line_profiler\n",
        "# %load_ext line_profiler\n",
        "# !git clone https://github.com/NVIDIA/PyProf.git\n",
        "# !pip install ./PyProf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4QoFYEQtFBZ"
      },
      "source": [
        "## Load Cython and Our Novel Efficient functions\n",
        "\n",
        "\n",
        "This cell contains the code we've implemented. You should be able to call each function directly, or alternatively, see our example calls below\n",
        "\n",
        "Cython allows us to call lower level c-code instead of using Python. It can be a surprisingly big speedup!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iQ3o3hbGZE1h"
      },
      "outputs": [],
      "source": [
        "\"\"\"This cell contains the code we've implemented. You should be able to call each function directly, or alternatively, see our example calls below\"\"\"\n",
        "\n",
        "from main import run_benchmarks\n",
        "\n",
        "n_components, n_features = 100, 100\n",
        "n_nonzero_coefs = 17\n",
        "n_samples = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Settings used for the test:\n",
            "\n",
            "Number of Components: 100\n",
            "Number of Features: 10\n",
            "Number of Nonzero Coefficients: 17\n",
            "GPU algorithms enabled: False\n",
            "Number of Samples: 50\n",
            "\n",
            "==================================================\n",
            "Testing problem size m = 16 (1/10)\n",
            "==================================================\n",
            "Sklearn OMP runtime: 0.0106\n",
            "Naive CPU runtime: 0.0103\n",
            "V0 CPU runtime: 0.0045\n",
            "Skipping GPU algorithms (run_gpu=False)\n",
            "\n",
            "============================== RECONSTRUCTION ERROR ==============================\n",
            "Sklearn reconstruction error: 2.257218e+01\n",
            "V0 CPU reconstruction error: 2.257217e+01\n",
            "Naive CPU reconstruction error: 2.257218e+01\n",
            "================================================================================\n",
            "\n",
            "==================================================\n",
            "Testing problem size m = 20 (2/10)\n",
            "==================================================\n",
            "Sklearn OMP runtime: 0.0087\n",
            "Naive CPU runtime: 0.0035\n",
            "V0 CPU runtime: 0.0097\n",
            "Skipping GPU algorithms (run_gpu=False)\n",
            "\n",
            "==================================================\n",
            "Testing problem size m = 24 (3/10)\n",
            "==================================================\n",
            "Sklearn OMP runtime: 0.0230\n",
            "Naive CPU runtime: 0.0107\n",
            "V0 CPU runtime: 0.0096\n",
            "Skipping GPU algorithms (run_gpu=False)\n",
            "\n",
            "==================================================\n",
            "Testing problem size m = 32 (4/10)\n",
            "==================================================\n",
            "Sklearn OMP runtime: 0.0221\n",
            "Naive CPU runtime: 0.0109\n",
            "V0 CPU runtime: 0.0101\n",
            "Skipping GPU algorithms (run_gpu=False)\n",
            "\n",
            "==================================================\n",
            "Testing problem size m = 64 (5/10)\n",
            "==================================================\n",
            "Sklearn OMP runtime: 0.0193\n",
            "Naive CPU runtime: 0.0084\n",
            "V0 CPU runtime: 0.0061\n",
            "Skipping GPU algorithms (run_gpu=False)\n",
            "\n",
            "==================================================\n",
            "Testing problem size m = 128 (6/10)\n",
            "==================================================\n",
            "Sklearn OMP runtime: 0.0111\n",
            "Naive CPU runtime: 0.0071\n",
            "V0 CPU runtime: 0.0091\n",
            "Skipping GPU algorithms (run_gpu=False)\n",
            "\n",
            "==================================================\n",
            "Testing problem size m = 256 (7/10)\n",
            "==================================================\n",
            "Sklearn OMP runtime: 0.0112\n",
            "Naive CPU runtime: 0.0069\n",
            "V0 CPU runtime: 0.0058\n",
            "Skipping GPU algorithms (run_gpu=False)\n",
            "\n",
            "==================================================\n",
            "Testing problem size m = 512 (8/10)\n",
            "==================================================\n",
            "Sklearn OMP runtime: 0.0112\n",
            "Naive CPU runtime: 0.0068\n",
            "V0 CPU runtime: 0.0057\n",
            "Skipping GPU algorithms (run_gpu=False)\n",
            "\n",
            "==================================================\n",
            "Testing problem size m = 1024 (9/10)\n",
            "==================================================\n",
            "Sklearn OMP runtime: 0.0121\n",
            "Naive CPU runtime: 0.0067\n",
            "V0 CPU runtime: 0.0060\n",
            "Skipping GPU algorithms (run_gpu=False)\n",
            "\n",
            "==================================================\n",
            "Testing problem size m = 2048 (10/10)\n",
            "==================================================\n",
            "Sklearn OMP runtime: 0.0124\n",
            "Naive CPU runtime: 0.0069\n",
            "V0 CPU runtime: 0.0058\n",
            "Skipping GPU algorithms (run_gpu=False)\n"
          ]
        }
      ],
      "source": [
        "execution_times = run_benchmarks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Optional: Line Profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Use this cell to profile line-by-line the algorithms you want to run\"\"\"\n",
        "\n",
        "from sklearn.datasets import make_sparse_coded_signal\n",
        "\n",
        "# m = 64\n",
        "# n_components, n_features = m*8, m\n",
        "# n_nonzero_coefs = m//4\n",
        "# n_samples = 200000\n",
        "\n",
        "\n",
        "# 20000 x 8000 x 1600 x 10 is just within memory reach on GPU\n",
        "n_components, n_features = 20000, 8000\n",
        "n_nonzero_coefs = 1600\n",
        "# Keep this above 1 not to mess with dimensions of y\n",
        "n_samples = 3\n",
        "\n",
        "# Out of memory on CPU if bigger than this\n",
        "# n_components, n_features = 20000, 8000\n",
        "# n_nonzero_coefs = 1000\n",
        "# n_samples = 10\n",
        "\n",
        "y, X, w = make_sparse_coded_signal(\n",
        "    n_samples=n_samples,\n",
        "    n_components=n_components,\n",
        "    n_features=n_features,\n",
        "    n_nonzero_coefs=n_nonzero_coefs,\n",
        "    random_state=0)\n",
        "\n",
        "y = y.T\n",
        "\n",
        "# Naive CPU\n",
        "# %lprun -f omp_naive run_omp(torch.as_tensor(X, device='cpu', dtype=torch.float), torch.as_tensor(y, device='cpu', dtype=torch.float), n_nonzero_coefs)\n",
        "# Naive GPU\n",
        "# %lprun -f omp_naive -f run_omp -f gpu_transfer_and_alg gpu_transfer_and_alg(X,y, \"naive\")\n",
        "\n",
        "\n",
        "# # V0 CPU\n",
        "# # %lprun -f omp_v0 run_omp(torch.as_tensor(X, device='cpu', dtype=torch.float), torch.as_tensor(y, device='cpu', dtype=torch.float), n_nonzero_coefs, alg=\"v0\")\n",
        "# # V0 GPU\n",
        "# %lprun -f omp_v0 -f run_omp -f gpu_transfer_and_alg gpu_transfer_and_alg(X,y, \"v0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "EZ_bFyYnpF4k",
        "outputId": "cc317d39-f6d5-4633-ba06-5e80948b2475"
      },
      "outputs": [],
      "source": [
        "\"\"\"Use this cell to get execution time as function of problem size\"\"\"\n",
        "\n",
        "execution_times = {}\n",
        "execution_times[\"sklearn\"] = []\n",
        "execution_times[\"naive_cpu\"] = []\n",
        "execution_times[\"v0_cpu\"] = []\n",
        "execution_times[\"naive_gpu\"] = []\n",
        "execution_times[\"v0_gpu\"] = []\n",
        "\n",
        "\n",
        "tol=0.1\n",
        "k=0\n",
        "\n",
        "# Big problems\n",
        "n_samples = 100\n",
        "m_arr = [16, 20, 24, 32, 64, 128, 256, 512, 1024, 2048]\n",
        "# m_arr = [2048]\n",
        "\n",
        "# Small problems\n",
        "# n_samples = 75000\n",
        "# m_arr = [8, 16, 24, 32, 64, 128]\n",
        "\n",
        "\n",
        "for m in m_arr:\n",
        "  n_components, n_features = m*8, m\n",
        "  n_nonzero_coefs = m//4\n",
        "\n",
        "  y, X, w = make_sparse_coded_signal(\n",
        "    n_samples=n_samples,\n",
        "    n_components=n_components,\n",
        "    n_features=n_features,\n",
        "    n_nonzero_coefs=n_nonzero_coefs,\n",
        "    random_state=2)\n",
        "\n",
        "  y = y.T\n",
        "\n",
        "  omp_args = dict(tol=tol, n_nonzero_coefs=n_nonzero_coefs-k, precompute=False, fit_intercept=True, normalize=True)\n",
        "  # Single core\n",
        "  print('Single core. Sklearn')\n",
        "  omp = OrthogonalMatchingPursuit(**omp_args)\n",
        "  with elapsed_timer() as elapsed:\n",
        "      omp.fit(X, y.T)\n",
        "  execution_times[\"sklearn\"].append(elapsed())\n",
        "\n",
        "  with elapsed_timer() as elapsed:\n",
        "    run_omp(torch.as_tensor(X, device='cpu', dtype=torch.float), torch.as_tensor(y, device='cpu', dtype=torch.float), n_nonzero_coefs)\n",
        "  execution_times[\"naive_cpu\"].append(elapsed())\n",
        "\n",
        "  with elapsed_timer() as elapsed:\n",
        "    run_omp(torch.as_tensor(X, device='cpu', dtype=torch.float), torch.as_tensor(y, device='cpu', dtype=torch.float), n_nonzero_coefs, alg=\"v0\")\n",
        "  execution_times[\"v0_cpu\"].append(elapsed())\n",
        "\n",
        "  with elapsed_timer() as elapsed:\n",
        "    gpu_transfer_and_alg(X,y, \"naive\")\n",
        "  execution_times[\"naive_gpu\"].append(elapsed())\n",
        "\n",
        "  with elapsed_timer() as elapsed:\n",
        "    gpu_transfer_and_alg(X,y, \"v0\")\n",
        "  execution_times[\"v0_gpu\"].append(elapsed())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmW1MJZggqOD"
      },
      "outputs": [],
      "source": [
        "execution_times"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "UqpbP4TIt3h2"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv-apr-2021",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
