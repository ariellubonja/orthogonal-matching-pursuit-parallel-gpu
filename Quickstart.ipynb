{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_rPkGOQ2x81",
        "outputId": "a6ea016e-0e6b-412c-f6c2-8c2b3d70d08b"
      },
      "outputs": [],
      "source": [
        "\"\"\"To profile the running time line-by-line, CUDA needs to run synchronously. Set this to 1 to do that\"\"\"\n",
        "%env CUDA_LAUNCH_BLOCKING=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXH6y_rftSeK"
      },
      "source": [
        "#### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpoUbaRkYaf9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\"\"\"For profiling how long each line takes, both on CPU and GPU (needs synchronization)\"\"\"\n",
        "!pip install line_profiler\n",
        "%load_ext line_profiler\n",
        "!git clone https://github.com/NVIDIA/PyProf.git\n",
        "!pip install ./PyProf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Novel Functions\n",
        "\n",
        "This cell contains the code we've implemented. You should be able to call each function directly, or alternatively, see our example calls below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4QoFYEQtFBZ"
      },
      "source": [
        "## Cython functions\n",
        "\n",
        "allows us to call lower level c-code instead of using Python. It can be a surprisingly big speedup!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGwIfcbTb6cR"
      },
      "outputs": [],
      "source": [
        "%load_ext Cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5ICpppib8dA"
      },
      "outputs": [],
      "source": [
        "%%cython\n",
        "\n",
        "\"\"\"Cython allows us to call lower level c-code instead of using Python. It can be a surprisingly big speedup!\"\"\"\n",
        "import cython\n",
        "from scipy.linalg.cython_blas cimport idamax, isamax, daxpy, dgemv, dtrmv, dcopy\n",
        "from scipy.linalg.cython_lapack cimport dposv, dppsv, sppsv\n",
        "\n",
        "ctypedef fused proj_t:\n",
        "    double\n",
        "    float\n",
        "\n",
        "@cython.boundscheck(False)\n",
        "@cython.wraparound(False)\n",
        "cpdef int ppsv(proj_t[:, :] As,\n",
        "           proj_t[:, :, :] ys) nogil:\n",
        "    # Works not for strided array I think. And please do not give a negative-stride array.\n",
        "    cdef Py_ssize_t B = ys.shape[0]  # Batch size\n",
        "    cdef int N = ys.shape[1]\n",
        "    cdef int nrhs = ys.shape[2]\n",
        "    cdef int info = 0  # Just discard any error signals ;)\n",
        "    cdef char uplo = 85 # The letter 'U', since we store the lower triangle and fortran sees As.T.\n",
        "    # cdef int ldb = ys[0].strides[0] // sizeof(double)\n",
        "\n",
        "    for i from 0 <= i < B:\n",
        "        if proj_t is double:  # One C-function is created for each of these specializations! :) (see argmax_blast.__signatures__)\n",
        "            dppsv(&uplo, &N, &nrhs, &As[i, 0], &ys[i, 0, 0], &N, &info)\n",
        "        elif proj_t is float:\n",
        "            sppsv(&uplo, &N, &nrhs, &As[i, 0], &ys[i, 0, 0], &N, &info)\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "@cython.boundscheck(False)\n",
        "@cython.wraparound(False)\n",
        "cpdef int argmax_blast(proj_t[:, :] projections,\n",
        "                 long long[:] output) nogil:\n",
        "    # TODO: Numpy has its own indexing data-type - this may be a more appropriate output, and may even be faster.\n",
        "    # http://conference.scipy.org/static/wiki/seljebotn_cython.pdf\n",
        "    # https://apprize.best/python/cython/3.html\n",
        "    cdef Py_ssize_t B = projections.shape[0]\n",
        "    cdef int N = projections.shape[1]\n",
        "    cdef int incx = projections.strides[1] // sizeof(proj_t)  # Stride between elements.\n",
        "    cdef Py_ssize_t i\n",
        "    for i from 0 <= i < B:\n",
        "        if proj_t is double:\n",
        "            output[i] = idamax(&N, &projections[i, 0], &incx) - 1\n",
        "        elif proj_t is float:\n",
        "            output[i] = isamax(&N, &projections[i, 0], &incx) - 1\n",
        "\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ3o3hbGZE1h"
      },
      "outputs": [],
      "source": [
        "\"\"\"This cell contains the code we've implemented. You should be able to call each function directly, or alternatively, see our example calls below\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.utils\n",
        "import torch.utils.data\n",
        "from sklearn.datasets import make_sparse_coded_signal\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
        "from contextlib import contextmanager\n",
        "from timeit import default_timer\n",
        "# from test_omp import omp_naive\n",
        "# from test import *  # FIXME: better name\n",
        "from line_profiler import line_profiler\n",
        "\n",
        "n_components, n_features = 100, 100\n",
        "n_nonzero_coefs = 17\n",
        "n_samples = 50\n",
        "\n",
        "@contextmanager\n",
        "def elapsed_timer():\n",
        "    # https://stackoverflow.com/questions/7370801/how-to-measure-elapsed-time-in-python\n",
        "    start = default_timer()\n",
        "    elapser = lambda: default_timer() - start\n",
        "    yield lambda: elapser()\n",
        "    end = default_timer()\n",
        "    elapser = lambda: end-start\n",
        "\n",
        "\n",
        "def run_omp(X, y, n_nonzero_coefs, precompute=True, tol=0.0, normalize=False, fit_intercept=False, alg='naive'):\n",
        "    if not isinstance(X, torch.Tensor):\n",
        "        X = torch.as_tensor(X)\n",
        "        y = torch.as_tensor(y)\n",
        "\n",
        "    # We can either return sets, (sets, solutions), or xests\n",
        "    # These are all equivalent, but are simply more and more dense representations.\n",
        "    # Given sets and X and y one can (re-)construct xests. The second is just a sparse vector repr.\n",
        "\n",
        "    # https://github.com/scikit-learn/scikit-learn/blob/15a949460dbf19e5e196b8ef48f9712b72a3b3c3/sklearn/linear_model/_omp.py#L690\n",
        "    if fit_intercept or normalize:\n",
        "        X = X.clone()\n",
        "        assert not isinstance(precompute, torch.Tensor), \"If user pre-computes XTX they can also pre-normalize X\" \\\n",
        "                                                         \" as well, so normalize and fit_intercept must be set false.\"\n",
        "\n",
        "    if fit_intercept:\n",
        "        X = X - X.mean(0)\n",
        "        y = y - y.mean(1)[:, None]\n",
        "\n",
        "    # To keep a good condition number on X, especially with Cholesky compared to LU factorization,\n",
        "    # we should probably always normalize it (OMP is invariant anyways)\n",
        "    if normalize is True:  # User can also just optionally supply pre-computed norms.\n",
        "        normalize = (X * X).sum(0).sqrt()\n",
        "        X /= normalize[None, :]\n",
        "\n",
        "    if precompute is True or alg == 'v0':\n",
        "        precompute = X.T @ X\n",
        "\n",
        "    # If n_nonzero_coefs is equal to M, one should just return lstsq\n",
        "    if alg == 'naive':\n",
        "        sets, solutions, lengths = omp_naive(X, y, n_nonzero_coefs=n_nonzero_coefs, XTX=precompute, tol=tol)\n",
        "    elif alg == 'v0':\n",
        "        sets, solutions, lengths = omp_v0(X, y, n_nonzero_coefs=n_nonzero_coefs, XTX=precompute, tol=tol)\n",
        "\n",
        "\n",
        "    solutions = solutions.squeeze(-1)\n",
        "    if normalize is not False:\n",
        "        solutions /= normalize[sets]\n",
        "\n",
        "    xests = y.new_zeros(y.shape[0], X.shape[1])\n",
        "    if lengths is None:\n",
        "        xests[torch.arange(y.shape[0], dtype=sets.dtype, device=sets.device)[:, None], sets] = solutions\n",
        "    else:\n",
        "        for i in range(y.shape[0]):\n",
        "            # xests[i].scatter_(-1, sets[i, :lengths[i]], solutions[i, :lengths[i]])\n",
        "            xests[i, sets[i, :lengths[i]]] = solutions[i, :lengths[i]]\n",
        "\n",
        "    return xests\n",
        "\n",
        "def batch_mm(matrix, matrix_batch, return_contiguous=True):\n",
        "    \"\"\"\n",
        "    :param matrix: Sparse or dense matrix, size (m, n).\n",
        "    :param matrix_batch: Batched dense matrices, size (b, n, k).\n",
        "    :return: The batched matrix-matrix product, size (m, n) x (b, n, k) = (b, m, k).\n",
        "    \"\"\"\n",
        "    # One dgemm is faster than many dgemv.\n",
        "    # From https://github.com/pytorch/pytorch/issues/14489#issuecomment-607730242\n",
        "    batch_size = matrix_batch.shape[0]\n",
        "    # Stack the vector batch into columns. (b, n, k) -> (n, b, k) -> (n, b*k)\n",
        "    vectors = matrix_batch.transpose([1, 0, 2]).reshape(matrix.shape[1], -1)\n",
        "\n",
        "    # A matrix-matrix product is a batched matrix-vector product of the columns.\n",
        "    # And then reverse the reshaping. (m, n) x (n, b*k) = (m, b*k) -> (m, b, k) -> (b, m, k)\n",
        "    if return_contiguous:\n",
        "        result = np.empty_like(matrix_batch, shape=(batch_size, matrix.shape[0], matrix_batch.shape[2]))\n",
        "        np.matmul(matrix, vectors, out=result.transpose([1, 0, 2]).reshape(matrix.shape[0], -1))\n",
        "    else:\n",
        "        result = (matrix @ vectors).reshape(matrix.shape[0], batch_size, -1).transpose([1, 0, 2])\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def innerp(x, y=None, out=None):\n",
        "    if y is None:\n",
        "        y = x\n",
        "    if out is not None:\n",
        "        out = out[:, None, None]  # Add space for two singleton dimensions.\n",
        "    return torch.matmul(x[..., None, :], y[..., :, None], out=out)[..., 0, 0]\n",
        "\n",
        "def cholesky_solve(ATA, ATy):\n",
        "    if ATA.dtype == torch.half or ATy.dtype == torch.half:\n",
        "        return ATy.to(torch.float).cholesky_solve(torch.cholesky(ATA.to(torch.float))).to(ATy.dtype)\n",
        "    return ATy.cholesky_solve(torch.cholesky(ATA)).to(ATy.dtype)\n",
        "\n",
        "\n",
        "def omp_naive(X, y, n_nonzero_coefs, tol=None, XTX=None):\n",
        "    on_cpu = not (y.is_cuda or y.dtype == torch.half)\n",
        "    # torch.cuda.synchronize()\n",
        "    # Given X as an MxN array and y as an BxN array, do omp to approximately solve Xb=y\n",
        "\n",
        "    # Base variables\n",
        "    XT = X.contiguous().t()  # Store XT in fortran-order.\n",
        "    y = y.contiguous()\n",
        "    r = y.clone()\n",
        "\n",
        "    sets = y.new_zeros((n_nonzero_coefs, y.shape[0]), dtype=torch.long).t()\n",
        "    if tol:\n",
        "        result_sets = sets.new_zeros(y.shape[0], n_nonzero_coefs)\n",
        "        result_lengths = sets.new_zeros(y.shape[0])\n",
        "        result_solutions = y.new_zeros((y.shape[0], n_nonzero_coefs, 1))\n",
        "        original_indices = torch.arange(y.shape[0], dtype=sets.dtype, device=sets.device)\n",
        "\n",
        "    # Trade b*k^2+bk+bkM = O(bkM) memory for much less compute time. (This has to be done anyways since we are batching,\n",
        "    # otherwise one could just permute columns of X in-place as in https://github.com/scikit-learn/scikit-learn/blob/15a949460dbf19e5e196b8ef48f9712b72a3b3c3/sklearn/linear_model/_omp.py#L28 )\n",
        "    ATs = y.new_zeros(r.shape[0], n_nonzero_coefs, X.shape[0])\n",
        "    ATys = y.new_zeros(r.shape[0], n_nonzero_coefs, 1)\n",
        "    ATAs = torch.eye(n_nonzero_coefs, dtype=y.dtype, device=y.device)[None].repeat(r.shape[0], 1, 1)\n",
        "    if on_cpu:\n",
        "        # For CPU it is faster to use a packed representation of the lower triangle in ATA.\n",
        "        tri_idx = torch.tril_indices(n_nonzero_coefs, n_nonzero_coefs, device=sets.device, dtype=sets.dtype)\n",
        "        ATAs = ATAs[:, tri_idx[0], tri_idx[1]]\n",
        "\n",
        "    solutions = y.new_zeros((r.shape[0], 0))\n",
        "\n",
        "    for k in range(n_nonzero_coefs+bool(tol)):\n",
        "        # STOPPING CRITERIA\n",
        "        if tol:\n",
        "            problems_done = innerp(r) <= tol\n",
        "            if k == n_nonzero_coefs:\n",
        "                problems_done[:] = True\n",
        "\n",
        "            if problems_done.any():\n",
        "                remaining = ~problems_done\n",
        "\n",
        "                orig_idxs = original_indices[problems_done]\n",
        "                result_sets[orig_idxs, :k] = sets[problems_done, :k]\n",
        "                result_solutions[orig_idxs, :k] = solutions[problems_done]\n",
        "                result_lengths[orig_idxs] = k\n",
        "                original_indices = original_indices[remaining]\n",
        "\n",
        "                # original_indices = original_indices[remaining]\n",
        "                ATs = ATs[remaining]\n",
        "                ATys = ATys[remaining]\n",
        "                ATAs = ATAs[remaining]\n",
        "                sets = sets[remaining]\n",
        "                y = y[remaining]\n",
        "                r = r[remaining]\n",
        "                if problems_done.all():\n",
        "                    return result_sets, result_solutions, result_lengths\n",
        "        # GET PROJECTIONS AND INDICES TO ADD\n",
        "        if on_cpu:\n",
        "            projections = batch_mm(XT.numpy(), r[:, :, None].numpy())\n",
        "            argmax_blast(projections.squeeze(-1), sets[:, k].numpy())\n",
        "        else:\n",
        "            projections = XT @ r[:, :, None]\n",
        "            sets[:, k] = projections.abs().sum(-1).argmax(-1)  # Sum is just a squeeze, but would be relevant in SOMP.\n",
        "\n",
        "        # UPDATE AT\n",
        "        AT = ATs[:, :k + 1, :]\n",
        "        updateA = XT[sets[:, k], :]\n",
        "        AT[:, k, :] = updateA\n",
        "\n",
        "        # UPDATE ATy based on AT\n",
        "        ATy = ATys[:, :k + 1]\n",
        "        innerp(updateA, y, out=ATy[:, k, 0])\n",
        "\n",
        "        # UPDATE ATA based on AT or precomputed XTX.\n",
        "        if on_cpu:\n",
        "            packed_idx = k * (k - 1) // 2\n",
        "            if XTX is not None:  # Update based on precomputed XTX.\n",
        "                ATAs.t()[k + packed_idx:packed_idx + 2 * k + 1, :].t().numpy()[:] = XTX[sets[:, k, None], sets[:, :k + 1]]\n",
        "            else:\n",
        "                np.matmul(AT[:, :k + 1, :].numpy(), updateA[:, :, None].numpy(),\n",
        "                          out=ATAs.t()[k + packed_idx:packed_idx + 2 * k + 1, :].t()[:, :, None].numpy())\n",
        "        else:\n",
        "            ATA = ATAs[:, :k + 1, :k + 1]\n",
        "            if XTX is not None:\n",
        "                ATA[:, k, :k + 1] = XTX[sets[:, k, None], sets[:, :k + 1]]\n",
        "            else:\n",
        "                # Update ATAs by adding the new column of inner products.\n",
        "                torch.bmm(AT[:, :k + 1, :], updateA[:, :, None], out=ATA[:, k, :k + 1, None])\n",
        "\n",
        "        # SOLVE ATAx = ATy.\n",
        "        if on_cpu:\n",
        "            solutions = ATy.permute(0, 2, 1).clone().permute(0, 2, 1)  # Get a copy.\n",
        "            ppsv(ATAs.t()[:packed_idx + 2 * k + 1, :].t().contiguous().numpy(), solutions.numpy())\n",
        "        else:\n",
        "            ATA[:, :k, k] = ATA[:, k, :k]  # Copy lower triangle to upper triangle.\n",
        "            solutions = cholesky_solve(ATA, ATy)\n",
        "\n",
        "        # FINALLY, GET NEW RESIDUAL r=y-Ax\n",
        "        if on_cpu:\n",
        "            np.subtract(y.numpy(), (AT.permute(0, 2, 1).numpy() @ solutions.numpy()).squeeze(-1), out=r.numpy())\n",
        "        else:\n",
        "            torch.baddbmm(y[:, :, None], AT.permute(0, 2, 1), solutions, beta=-1, out=r[:, :, None])\n",
        "\n",
        "    return sets, solutions, None\n",
        "\n",
        "def omp_v0(X, y, XTX, n_nonzero_coefs=None, tol=None, inverse_cholesky=True):\n",
        "    B = y.shape[0]\n",
        "    normr2 = innerp(y)  # Norm squared of residual.\n",
        "    projections = (X.transpose(1, 0) @ y[:, :, None]).squeeze(-1)\n",
        "    sets = y.new_zeros(n_nonzero_coefs, B, dtype=torch.int64)\n",
        "\n",
        "    if inverse_cholesky:\n",
        "        # Doing the inverse-cholesky iteratively uses more memory,\n",
        "        # but takes less time than waiting till solving the problem in the end it seems.\n",
        "        # (Since F is triangular it could be __even faster__ to multiply, prob. not on GPU tho.)\n",
        "        F = torch.eye(n_nonzero_coefs, dtype=y.dtype, device=y.device).repeat(B, 1, 1)\n",
        "        a_F = y.new_zeros(n_nonzero_coefs, B, 1)\n",
        "\n",
        "    D_mybest = y.new_empty(B, n_nonzero_coefs, XTX.shape[0])\n",
        "    temp_F_k_k = y.new_ones((B, 1))\n",
        "\n",
        "    if tol:\n",
        "        result_lengths = sets.new_zeros(y.shape[0])\n",
        "        result_solutions = y.new_zeros((y.shape[0], n_nonzero_coefs, 1))\n",
        "        finished_problems = sets.new_zeros(y.shape[0], dtype=torch.bool)\n",
        "\n",
        "    for k in range(n_nonzero_coefs+bool(tol)):\n",
        "        # STOPPING CRITERIA\n",
        "        if tol:\n",
        "            problems_done = normr2 <= tol\n",
        "            if k == n_nonzero_coefs:\n",
        "                problems_done[:] = True\n",
        "\n",
        "            if problems_done.any():\n",
        "                new_problems_done = problems_done & ~finished_problems\n",
        "                finished_problems.logical_or_(problems_done)\n",
        "                result_lengths[new_problems_done] = k\n",
        "                if inverse_cholesky:\n",
        "                    result_solutions[new_problems_done, :k] = F[new_problems_done, :k, :k].permute(0, 2, 1) @ a_F[:k, new_problems_done].permute(1, 0, 2)\n",
        "                else:\n",
        "                    assert False, \"inverse_cholesky=False with tol != None is not handled yet\"\n",
        "                if problems_done.all():\n",
        "                    return sets.t(), result_solutions, result_lengths\n",
        "\n",
        "        sets[k] = projections.abs().argmax(1)\n",
        "        # D_mybest[:, k, :] = XTX[gamma[k], :]  # Same line as below, but significantly slower. (prob. due to the intermediate array creation)\n",
        "        torch.gather(XTX, 0, sets[k, :, None].expand(-1, XTX.shape[1]), out=D_mybest[:, k, :])\n",
        "        if k:\n",
        "            D_mybest_maxindices = D_mybest.permute(0, 2, 1)[torch.arange(D_mybest.shape[0], dtype=sets.dtype, device=sets.device), sets[k], :k]\n",
        "            torch.rsqrt(1 - innerp(D_mybest_maxindices),\n",
        "                        out=temp_F_k_k[:, 0])  # torch.exp(-1/2 * torch.log1p(-inp), temp_F_k_k[:, 0])\n",
        "            D_mybest_maxindices *= -temp_F_k_k  # minimal operations, exploit linearity\n",
        "            D_mybest[:, k, :] *= temp_F_k_k\n",
        "            D_mybest[:, k, :, None].baddbmm_(D_mybest[:, :k, :].permute(0, 2, 1), D_mybest_maxindices[:, :, None])\n",
        "\n",
        "\n",
        "        temp_a_F = temp_F_k_k * torch.gather(projections, 1, sets[k, :, None])\n",
        "        normr2 -= (temp_a_F * temp_a_F).squeeze(-1)\n",
        "        projections -= temp_a_F * D_mybest[:, k, :]\n",
        "        if inverse_cholesky:\n",
        "            a_F[k] = temp_a_F\n",
        "            if k:  # Could maybe get a speedup from triangular mat mul kernel.\n",
        "                torch.bmm(D_mybest_maxindices[:, None, :], F[:, :k, :], out=F[:, k, None, :])\n",
        "                F[:, k, k] = temp_F_k_k[..., 0]\n",
        "    else: # FIXME: else branch will not execute if n_nonzero_coefs=0, so solutions is undefined.\n",
        "        # Normal exit, used when tolerance=None.\n",
        "        if inverse_cholesky:\n",
        "            solutions = F.permute(0, 2, 1) @ a_F.squeeze(-1).transpose(1, 0)[:, :, None]\n",
        "        else:\n",
        "            # Solving the problem in the end without using inverse Cholesky.\n",
        "            AT = X.T[sets.T]\n",
        "            solutions = cholesky_solve(AT @ AT.permute(0, 2, 1), AT @ y.T[:, :, None])\n",
        "\n",
        "    return sets.t(), solutions, None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqpbP4TIt3h2"
      },
      "source": [
        "## Speedup Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1NSaieCvcr5"
      },
      "outputs": [],
      "source": [
        "\"\"\"This is a helper function for allowing us to profile both GPU runtime as well as the time it takes to transfer to GPU memory\"\"\"\n",
        "def gpu_transfer_and_alg(X,y, alg):\n",
        "    X_gpu = torch.as_tensor(X, device='cuda', dtype=torch.float)\n",
        "    y_gpu = torch.as_tensor(y, device='cuda', dtype=torch.float)\n",
        "    results = run_omp(X_gpu, y_gpu, n_nonzero_coefs, alg=alg)\n",
        "    results.cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqzwoCiTCNEN"
      },
      "source": [
        "### Line-Based Profiling - `lprun`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpHMClZOjQUZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"Use this cell to profile line-by-line the algorithms you want to run\"\"\"\n",
        "\n",
        "from sklearn.datasets import make_sparse_coded_signal\n",
        "\n",
        "# m = 64\n",
        "# n_components, n_features = m*8, m\n",
        "# n_nonzero_coefs = m//4\n",
        "# n_samples = 200000\n",
        "\n",
        "\n",
        "# 20000 x 8000 x 1600 x 10 is just within memory reach on GPU\n",
        "n_components, n_features = 20000, 8000\n",
        "n_nonzero_coefs = 1600\n",
        "# Keep this above 1 not to mess with dimensions of y\n",
        "n_samples = 3\n",
        "\n",
        "# Out of memory on CPU if bigger than this\n",
        "# n_components, n_features = 20000, 8000\n",
        "# n_nonzero_coefs = 1000\n",
        "# n_samples = 10\n",
        "\n",
        "y, X, w = make_sparse_coded_signal(\n",
        "    n_samples=n_samples,\n",
        "    n_components=n_components,\n",
        "    n_features=n_features,\n",
        "    n_nonzero_coefs=n_nonzero_coefs,\n",
        "    random_state=0)\n",
        "\n",
        "y = y.T\n",
        "\n",
        "# Naive CPU\n",
        "# %lprun -f omp_naive run_omp(torch.as_tensor(X, device='cpu', dtype=torch.float), torch.as_tensor(y, device='cpu', dtype=torch.float), n_nonzero_coefs)\n",
        "# Naive GPU\n",
        "# %lprun -f omp_naive -f run_omp -f gpu_transfer_and_alg gpu_transfer_and_alg(X,y, \"naive\")\n",
        "\n",
        "\n",
        "# # V0 CPU\n",
        "# # %lprun -f omp_v0 run_omp(torch.as_tensor(X, device='cpu', dtype=torch.float), torch.as_tensor(y, device='cpu', dtype=torch.float), n_nonzero_coefs, alg=\"v0\")\n",
        "# # V0 GPU\n",
        "# %lprun -f omp_v0 -f run_omp -f gpu_transfer_and_alg gpu_transfer_and_alg(X,y, \"v0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ_bFyYnpF4k",
        "outputId": "cf544c8c-0444-47dc-a836-e3fece596270"
      },
      "outputs": [],
      "source": [
        "\"\"\"Use this cell to get execution time as function of problem size\"\"\"\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "execution_times = {}\n",
        "execution_times[\"sklearn\"] = []\n",
        "execution_times[\"naive_cpu\"] = []\n",
        "execution_times[\"v0_cpu\"] = []\n",
        "execution_times[\"naive_gpu\"] = []\n",
        "execution_times[\"v0_gpu\"] = []\n",
        "\n",
        "\n",
        "tol=0.1\n",
        "k=0\n",
        "\n",
        "####### Various Problem Sizes #######\n",
        "n_samples = 100\n",
        "m_arr = [16, 20, 24, 32, 64]#, 128, 256, 512, 1024, 2048]\n",
        "\n",
        "\n",
        "for m in m_arr:\n",
        "  n_components, n_features = m*8, m\n",
        "  n_nonzero_coefs = m//4\n",
        "\n",
        "  y, _, X = make_sparse_coded_signal(\n",
        "    n_samples=n_samples,\n",
        "    n_components=n_components,\n",
        "    n_features=n_features,\n",
        "    n_nonzero_coefs=n_nonzero_coefs,\n",
        "    random_state=2)\n",
        "\n",
        "  y = y.T\n",
        "\n",
        "  # Newer versions of Sklearn do not offer a `normalize` argument -- they assume X columns are unit-norm\n",
        "  X = normalize(X, axis=1, norm='l2')\n",
        "\n",
        "  omp_args = dict(tol=tol, n_nonzero_coefs=n_nonzero_coefs-k, precompute=False, fit_intercept=True)\n",
        "\n",
        "#   print('Single core. Sklearn')\n",
        "  omp = OrthogonalMatchingPursuit(**omp_args)\n",
        "  with elapsed_timer() as elapsed:\n",
        "      omp.fit(X, y.T)\n",
        "  execution_times[\"sklearn\"].append(elapsed())\n",
        "\n",
        "  with elapsed_timer() as elapsed:\n",
        "    run_omp(torch.as_tensor(X, device='cpu', dtype=torch.float), torch.as_tensor(y, device='cpu', dtype=torch.float), n_nonzero_coefs)\n",
        "  execution_times[\"naive_cpu\"].append(elapsed())\n",
        "\n",
        "  with elapsed_timer() as elapsed:\n",
        "    run_omp(torch.as_tensor(X, device='cpu', dtype=torch.float), torch.as_tensor(y, device='cpu', dtype=torch.float), n_nonzero_coefs, alg=\"v0\")\n",
        "  execution_times[\"v0_cpu\"].append(elapsed())\n",
        "\n",
        "#   with elapsed_timer() as elapsed:\n",
        "#     gpu_transfer_and_alg(X,y, \"naive\")\n",
        "#   execution_times[\"naive_gpu\"].append(elapsed())\n",
        "\n",
        "#   with elapsed_timer() as elapsed:\n",
        "#     gpu_transfer_and_alg(X,y, \"v0\")\n",
        "#   execution_times[\"v0_gpu\"].append(elapsed())\n",
        "\n",
        "execution_times\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5MqRzxcbMtW",
        "outputId": "7c877a8c-d436-4ebc-8191-69adc7085a2e"
      },
      "outputs": [],
      "source": [
        "# Plot execution time as function of problem size\n",
        "\n",
        "# m = 64\n",
        "# n_components, n_features = m*8, m\n",
        "# n_nonzero_coefs = m//4\n",
        "n_samples = 100\n",
        "\n",
        "tol=0.1\n",
        "k=0\n",
        "\n",
        "m_arr = [16, 32, 64, 128, 256, 512, 1024]\n",
        "\n",
        "time_spent = []\n",
        "\n",
        "for m in m_arr:\n",
        "  n_components, n_features = m*8, m\n",
        "  n_nonzero_coefs = m//4\n",
        "\n",
        "  y, _, X = make_sparse_coded_signal(\n",
        "    n_samples=n_samples,\n",
        "    n_components=n_components,\n",
        "    n_features=n_features,\n",
        "    n_nonzero_coefs=n_nonzero_coefs,\n",
        "    random_state=0)\n",
        "\n",
        "  X = normalize(X, axis=1, norm='l2')\n",
        "\n",
        "  y = y.T\n",
        "\n",
        "  omp_args = dict(tol=tol, n_nonzero_coefs=n_nonzero_coefs-k, precompute=False, fit_intercept=True)\n",
        "\n",
        "  # Sklearn's OMP impl.\n",
        "  omp = OrthogonalMatchingPursuit(**omp_args)\n",
        "  with elapsed_timer() as elapsed:\n",
        "      omp.fit(X, y.T)\n",
        "  time_spent.append(elapsed())\n",
        "\n",
        "execution_times[\"sklearn\"] = time_spent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7cUytw-oG88"
      },
      "outputs": [],
      "source": [
        "\"\"\"This and the cells below are to help figure out what hardware COLAB has assigned to our current runtime. It is mostly for curiosity reasons\"\"\"\n",
        "!nvidia-smi -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfCOKVVVnBCq"
      },
      "outputs": [],
      "source": [
        "!lscpu | grep -E \"Hz|cache|Core|Socket\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnTpG6zFnCQI"
      },
      "outputs": [],
      "source": [
        "!cat /proc/meminfo | grep MemTotal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2Z9UbIBLCXU"
      },
      "outputs": [],
      "source": [
        "from scipy.io import loadmat\n",
        "yale_faces = loadmat(\"/content/CroppedYale_96_84_2414_subset.mat\")\n",
        "yale_faces\n",
        "faces = yale_faces[\"faces\"]\n",
        "facecls = yale_faces[\"facecls\"]\n",
        "num_images = len(facecls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnhWriKALqtc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.shape(yale_faces[\"faces\"][0,:,:])\n",
        "height = np.shape(yale_faces[\"faces\"][0,:,:])[0]\n",
        "width = np.shape(yale_faces[\"faces\"][0,:,:])[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t8q6uz4MSc3"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.imshow(yale_faces[\"faces\"][0,:,:], cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUqsjaZiOWRv"
      },
      "outputs": [],
      "source": [
        "# % Uniform sampling images to form training set\n",
        "# % Split train and test 50-50\n",
        "np.random.permutation(num_images)\n",
        "training_set_indexes = np.random.permutation(num_images)[:num_images//2]\n",
        "print(len(training_set_indexes))\n",
        "test_set_indexes = np.random.permutation(num_images)[num_images//2:]\n",
        "print(len(test_set_indexes))\n",
        "\n",
        "training_set = faces[training_set_indexes,:,:]\n",
        "labels_for_training_set = facecls[training_set_indexes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQZeuEsHNAmv"
      },
      "outputs": [],
      "source": [
        "# % Forming giant A dictionary\n",
        "\n",
        "# %double(imresize(Test_Image,[m1 n1]))\n",
        "A = np.zeros((height*width, num_images//2));\n",
        "# for tra_img in training_set:\n",
        "for i in range(num_images//2):\n",
        "    # print(np.shape(training_set(i,:,:)))\n",
        "    col_img=np.reshape(training_set[i,:,:],(height*width, 1));\n",
        "    # col_img = double(col_img);\n",
        "    # A = np.hstack((A,col_img));\n",
        "    A[:,i] = np.squeeze(col_img)\n",
        "# disp(\"Size of A\")\n",
        "A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeTPs5J3r5od"
      },
      "outputs": [],
      "source": [
        "# Make sure that the above transformatinos\n",
        "plt.imshow(np.reshape(A[:,i], (height, width)), cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWuPwyhQ2fEB"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "norm_A = normalize(A, axis=1, norm='max')\n",
        "norm_A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqM_nTVbsNge"
      },
      "outputs": [],
      "source": [
        "# % Run algorithm for just 1 test image\n",
        "from scipy.linalg import norm\n",
        "\n",
        "print(\"For now, test with the 200th test_set example\")\n",
        "test_img_1 = faces[test_set_indexes[300],:,:]\n",
        "# disp(\"Test image\")\n",
        "# plt.imshow(test_img_1, cmap=\"gray\")\n",
        "flat_test_img_1 = np.reshape(test_img_1,(height*width,1))\n",
        "norm_flat = normalize(flat_test_img_1, axis=0, norm='max')\n",
        "# flat_test_img_1 = double(flat_test_img_1);\n",
        "\n",
        "x = run_omp(torch.as_tensor(norm_A, device='cpu', dtype=torch.float), torch.as_tensor(norm_flat.T, device='cpu', dtype=torch.float), 30)\n",
        "\n",
        "# x = OMP(flat_test_img_1, A, 30);\n",
        "# omp_args = dict(tol=1e-6, n_nonzero_coefs=30, precompute=False, fit_intercept=False, normalize=False)\n",
        "# # Single core\n",
        "# print('Single core. Sklearn')\n",
        "# omp = OrthogonalMatchingPursuit(**omp_args)\n",
        "# with elapsed_timer() as elapsed:\n",
        "#     omp.fit(norm_A, norm_flat)\n",
        "# execution_times[\"sklearn\"].append(elapsed())\n",
        "\n",
        "print(omp.coef_)\n",
        "x = omp.coef_\n",
        "print(np.shape(x))\n",
        "plt.plot(x)\n",
        "print(np.argmax(x))\n",
        "print(\"Predicted img class\", facecls[training_set_indexes[np.argmax(x)]])\n",
        "print(\"True img class\", facecls[test_set_indexes[300]])\n",
        "\n",
        "\n",
        "# TODO Implement proper residual calculation\n",
        "# residual_for_class = np.zeros((38,1));\n",
        "# for class_number in range(1,39):\n",
        "#     residual_for_class[class_number-1] = norm(flat_test_img_1 - A[:, np.squeeze(labels_for_training_set.T == class_number)] @ x[np.squeeze(labels_for_training_set.T == class_number)])\n",
        "\n",
        "# # disp(\"Residual plot for first test image\")\n",
        "# plt.plot(range(1, 39), residual_for_class)\n",
        "# [resid cls_pick] = min(residual_for_class);\n",
        "# disp(\"Predicted image class: \" + cls_pick);\n",
        "# disp(\"True image class\")\n",
        "# facecls(test_set_indexes(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Dgg6l1X2RzE"
      },
      "outputs": [],
      "source": [
        "# Batch all test images together, to make them ready for our algs.\n",
        "test_images_batch = np.zeros((height*width, len(test_set_indexes)))\n",
        "count = 0\n",
        "for i in test_set_indexes:\n",
        "  test_img_1 = faces[i,:,:]\n",
        "  flat_test_img_1 = np.reshape(test_img_1,(height*width,1))\n",
        "  norm_flat = normalize(flat_test_img_1, axis=0, norm='max')\n",
        "  test_images_batch[:,count] = np.squeeze(norm_flat)\n",
        "  count+=1\n",
        "# flat_test_img_1 = double(flat_test_img_1);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vhHzodt-CNu"
      },
      "outputs": [],
      "source": [
        "np.shape(test_images_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4hmX1Hg-5PG"
      },
      "outputs": [],
      "source": [
        "# HW7 batched test images on Sklearn\n",
        "omp_args = dict(tol=1e-6, n_nonzero_coefs=30, precompute=False, fit_intercept=False, normalize=False)\n",
        "\n",
        "print('Single core. Sklearn')\n",
        "omp = OrthogonalMatchingPursuit(**omp_args)\n",
        "with elapsed_timer() as elapsed:\n",
        "    omp.fit(norm_A, test_images_batch)\n",
        "elapsed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCPFntJn-GGt"
      },
      "outputs": [],
      "source": [
        "# HW7 batched test images on Naive\n",
        "with elapsed_timer() as elapsed:\n",
        "  xes = run_omp(torch.as_tensor(norm_A, device='cpu', dtype=torch.float), torch.as_tensor(test_images_batch.T, device='cpu', dtype=torch.float), 30)\n",
        "elapsed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "149v7b7d-QHE"
      },
      "outputs": [],
      "source": [
        "# HW7 batched test images on V0\n",
        "with elapsed_timer() as elapsed:\n",
        "  xes = run_omp(torch.as_tensor(norm_A, device='cpu', dtype=torch.float), torch.as_tensor(test_images_batch.T, device='cpu', dtype=torch.float), 30, alg=\"v0\")\n",
        "elapsed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJRxA0kx-u4b"
      },
      "outputs": [],
      "source": [
        "# HW7 batched test images on Naive GPU\n",
        "with elapsed_timer() as elapsed:\n",
        "  xes = run_omp(torch.as_tensor(norm_A, device='cuda', dtype=torch.float), torch.as_tensor(test_images_batch.T, device='cuda', dtype=torch.float), 30)\n",
        "elapsed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewq6sFZjHosR"
      },
      "outputs": [],
      "source": [
        "# HW7 batched test images on V0 GPU\n",
        "with elapsed_timer() as elapsed:\n",
        "  xes = run_omp(torch.as_tensor(norm_A, device='cuda', dtype=torch.float), torch.as_tensor(test_images_batch.T, device='cuda', dtype=torch.float), 30, alg=\"v0\")\n",
        "elapsed()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
